{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Zcash Observatory data\n",
    "\n",
    "Mitchell Krawiec-Thayer and Pranav Thirunavukkarasu\n",
    "\n",
    "June 2020\n",
    "\n",
    "Observatory R & D at Insight supported by the Zcash Foundation\n",
    "\n",
    "\n",
    "Expects directory structure:\n",
    "\n",
    "```\n",
    "/\n",
    "|\n",
    "├── *** THIS NOTEBOOK ***\n",
    "|\n",
    "├── africa/\n",
    "│   ├── blocks_v1.csv\n",
    "│   └── inv_v1.csv\n",
    "|\n",
    "├── london/\n",
    "│   ├── blocks_v1.csv\n",
    "│   └── inv_v1.csv\n",
    "|\n",
    "├── mumbai/\n",
    "│   ├── blocks_v1.csv\n",
    "│   └── inv_v1.csv\n",
    "|\n",
    "└── virginia/\n",
    "    ├── blocks_v1.csv\n",
    "    └── inv_v1.csv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiments\n",
    "start_height =  863494\n",
    "stop_height = 863494\n",
    "min_height = 0 # 863494 - 48*48 # set to 0 to disable\n",
    "median_window = 48*4 # 10 hours\n",
    "percentile_val = 98\n",
    "\n",
    "### General Settings\n",
    "path_to_files = '.'\n",
    "savedata = 1 # boolean setting\n",
    "qverbose = 2 # boolean setting\n",
    "\n",
    "### Random stuff\n",
    "remove_sync_hack = 0 # deprecated\n",
    "min_obs_witness = 1 # Mask height unless witnessed by this number of observatories\n",
    "sync_threshold = 0 # seconds interval for block inv messages\n",
    "envelope_cutoff = -9999 # max prop time, anything longer sync # not implemented yet\n",
    "block_time = 75 # seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pandas as pd;\n",
    "import math;\n",
    "import os;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates heatmap visualizations\n",
    "\n",
    "def Heatmap(x, y, LinBins = (60,60), LogBins = (60,60), title = '', xlabel = '', ylabel = '', yscale = 'linear', xscale = 'linear', onlyplot = '', vmax = 'auto', vmin = 0, clabel='', ymax = 'auto'):\n",
    "    # import numpy as np\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # MPKT 2019.06\n",
    "\n",
    "    # Note that this is a hacky function that cannot handle NaNs at the moment\n",
    "    \n",
    "    if ymax == 'auto':\n",
    "        ymax_val = np.log10(int(np.max(x)))\n",
    "    else:\n",
    "        ymax_val = np.log10(int(ymax))\n",
    "    \n",
    "    # Log plot\n",
    "    if not onlyplot == 'linear':\n",
    "        fig = plt.figure(figsize=(15,5), facecolor='white')\n",
    "        if len(LogBins) == 2:\n",
    "            yedges = np.logspace(np.log10(1),ymax_val, LogBins[1])\n",
    "        H, xedges, yedges = np.histogram2d(list(x),list(y), bins=(LogBins[1],yedges))\n",
    "        \n",
    "        # H needs to be rotated and flipped\n",
    "        H = np.rot90(H)\n",
    "        H = np.flipud(H)\n",
    "        \n",
    "        if vmax == 'auto':\n",
    "            vmax = np.max(H)\n",
    "        # Plot 2D histogram using pcolor\n",
    "        plt.pcolormesh(xedges,yedges,H,vmax=vmax, vmin=vmin)\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.ax.set_ylabel(clabel)                                          \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.xscale(xscale)\n",
    "        plt.yscale(yscale)\n",
    "        plt.show()\n",
    "        \n",
    "        return fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Assumes that data is in directories named by node ID (location)\n",
    "\n",
    "The cell that reads in CSV files also heavily processes, and may take minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_names:\n",
      "['mumbai', 'virginia', 'london', 'africa']\n"
     ]
    }
   ],
   "source": [
    "# What are the nearby directories\n",
    "folders = [x[0] for x in os.walk(path_to_files)]\n",
    "\n",
    "# Initialize\n",
    "node_names = list()\n",
    "for f in range(len(folders)):\n",
    "    this_folder_raw = folders[f]\n",
    "    if not (this_folder_raw[0:3] == './.' or this_folder_raw == '.'):\n",
    "        node_names.append(this_folder_raw[2::])\n",
    "print('node_names:')\n",
    "print (node_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data from all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data, iterating over nodes\n",
    "\n",
    "timestamps_df = pd.DataFrame()\n",
    "heights_df = pd.DataFrame()\n",
    "actual_node_names = ()\n",
    "\n",
    "for f in range(len(node_names)):\n",
    "    this_node_name = node_names[f]\n",
    "\n",
    "\n",
    "    # Import height data\n",
    "    blocks_file_name = os.path.join(this_node_name,'blocks_v1.csv')\n",
    "    temp_df1 = pd.read_csv(blocks_file_name, index_col = 'Hash')\n",
    "    temp_df1 = temp_df1.filter([\"Hash\", \"Height\"], axis=1)\n",
    "    temp_df1['Obs_Node_Name'] = this_node_name\n",
    "    heights_df = heights_df.append(temp_df1)\n",
    "\n",
    "    # Import timestamp data\n",
    "    heights_file_name = os.path.join(this_node_name,'inv_v1.csv')\n",
    "    temp_df2 = pd.read_csv(heights_file_name, index_col = 'Hash')\n",
    "    timestamps_df = timestamps_df.append(temp_df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join heights\n",
    "all_data = timestamps_df.join(heights_df.drop_duplicates(), on='Hash') # temp hook\n",
    "\n",
    "# Filter by height\n",
    "all_data = all_data[all_data['Height']>min_height]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer piecewise analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all peers that we saw?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_peerlist = list(set(all_data['Peer_IP']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove sync data (loop over peers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peer 1 of 259\n",
      "Peer 11 of 259\n",
      "Peer 21 of 259\n",
      "Peer 31 of 259\n",
      "Peer 41 of 259\n",
      "Peer 51 of 259\n",
      "Peer 61 of 259\n",
      "Peer 71 of 259\n",
      "Peer 81 of 259\n",
      "Peer 91 of 259\n",
      "Peer 101 of 259\n",
      "Peer 111 of 259\n",
      "Peer 121 of 259\n",
      "Peer 131 of 259\n",
      "Peer 141 of 259\n",
      "Peer 151 of 259\n",
      "Peer 161 of 259\n",
      "Peer 171 of 259\n",
      "Peer 181 of 259\n",
      "Peer 191 of 259\n",
      "Peer 201 of 259\n",
      "Peer 211 of 259\n",
      "Peer 221 of 259\n"
     ]
    }
   ],
   "source": [
    "heights_to_keep = list()\n",
    "heights_to_mask = list()\n",
    "non_sync_data = pd.DataFrame()\n",
    "\n",
    "for p in range(len(global_peerlist)):\n",
    "    if qverbose: \n",
    "        if p % 10 == 0:\n",
    "            print(\"Peer \" + str(p+1) + ' of ' + str(len(global_peerlist)))\n",
    "    this_IP = global_peerlist[p]\n",
    "    this_peer_data = all_data[all_data['Peer_IP'] == this_IP]\n",
    "    this_peer_data = this_peer_data.reset_index()\n",
    "    heights_seen_raw = list(set(this_peer_data['Height']))\n",
    "    heights_seen = [x for x in heights_seen_raw if not np.isnan(x)]\n",
    "    \n",
    "    # De-dupe heights\n",
    "    first_heard_df = this_peer_data.groupby(['Height'])['Validated_Time'].min().to_frame().reset_index()\n",
    "    \n",
    "    # Split into parallel lists\n",
    "    height_array = list(first_heard_df[\"Height\"])\n",
    "    time_array = list(first_heard_df[\"Validated_Time\"])\n",
    "\n",
    "    for h in range(len(height_array)):\n",
    "        # get data for this block\n",
    "        this_height = height_array[h]\n",
    "        this_time = time_array[h]\n",
    "        \n",
    "        # did the previous block exist\n",
    "        last_height = this_height - 1\n",
    "        try:\n",
    "            last_height_index = height_array.index(last_height)\n",
    "            last_time = time_array[last_height_index]\n",
    "            \n",
    "            if this_time - last_time > sync_threshold:\n",
    "                heights_to_keep.append(this_height)\n",
    "            else:\n",
    "                # previous block was too recent\n",
    "                heights_to_mask.append(this_height)\n",
    "        except:\n",
    "            # didn't see previous block\n",
    "            heights_to_mask.append(this_height)\n",
    "            \n",
    "\n",
    "    # Okay, now remove those rows from the all-node this-peer data \n",
    "    for h in range(len(heights_to_mask)):\n",
    "        this_height = heights_to_mask[h]\n",
    "        idx = this_peer_data.index[this_peer_data['Height'] == this_height].tolist()\n",
    "        \n",
    "        if len(idx) > 0:\n",
    "            if len(idx) > 1:\n",
    "                for i in range(len(idx)):\n",
    "                    this_peer_data.iat[i,1] = np.nan \n",
    "            else: # len(idx) == 1:\n",
    "                this_peer_data.iat[idx[0],1] = np.nan \n",
    "\n",
    "    this_peer_data = this_peer_data.dropna()\n",
    "    \n",
    "    non_sync_data = non_sync_data.append(this_peer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(columns=['Height','Hash','First_Time','Prop_Time','Obs_Witness_Count','Peers_Report_Count','Sorted_Timestamps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "witness_df = pd.DataFrame()\n",
    "global_height_list_raw = list(set(all_data['Height']))\n",
    "global_height_list = [x for x in global_height_list_raw if not np.isnan(x)]\n",
    "summary_df = pd.DataFrame(columns=['Height','Hash','First_Time','Prop_Time','Obs_Witness_Count','Peers_Report_Count','Sorted_Timestamps'])\n",
    "\n",
    "height_col = list()\n",
    "hash_col = list()\n",
    "first_col = list()\n",
    "prop_time_col = list()\n",
    "obs_wit_col = list()\n",
    "peers_col = list()\n",
    "timestamps_col = list()\n",
    "prct_prop_time_col = list()\n",
    "\n",
    "for h in range(len(global_height_list)):\n",
    "    if qverbose > 0: \n",
    "        if h % 250 == 0:\n",
    "            print(str(h) + ' of ' + str(len(global_height_list)))\n",
    "    this_height = global_height_list[h]\n",
    "    this_height_df = non_sync_data[non_sync_data[\"Height\"] == this_height]\n",
    "    height_col.append(this_height)\n",
    "    # hash_col.append()\n",
    "        \n",
    "    try: \n",
    "        raw_times = this_height_df['Validated_Time']\n",
    "        min_time = min(raw_times) # get the min time \n",
    "        max_allowed = min_time + block_time \n",
    "        allowed_times = raw_times[raw_times<max_allowed]   \n",
    "        max_time = max(allowed_times)\n",
    "        prop_time = max_time - min_time       \n",
    "        prct_prop_time = np.percentile(allowed_times,percentile_val)-min_time\n",
    "        if qverbose > 2: print('***')\n",
    "        if qverbose > 2: print('Height: ' + str(this_height))\n",
    "        if qverbose > 2: print('Raw times: ' + str(raw_times))  \n",
    "        if qverbose > 2: print('Min time: '+ str(min_time))\n",
    "        if qverbose > 2: print('Max allowed: ' + str(max_allowed))\n",
    "        if qverbose > 2: print('Allowed times: ' + str(allowed_times))  \n",
    "        if qverbose > 2: print('Max time: ' + str(max_time))\n",
    "        if qverbose > 2: print('Prop time: ' + str(prop_time))\n",
    "        if qverbose > 2: print('Prct prop time: ' + str(prct_prop_time))\n",
    "\n",
    "        \n",
    "    except:\n",
    "        min_time = np.nan\n",
    "        prop_time = np.nan\n",
    "        prct_prop_time = np.nan\n",
    "        if qverbose > 2: print('Except!')\n",
    "\n",
    "                \n",
    "    first_col.append(min_time)\n",
    "    prop_time_col.append(prop_time)\n",
    "    prct_prop_time_col.append(prct_prop_time)\n",
    "    \n",
    "    # Count witnesses\n",
    "    obs_witnesses = list(set(this_height_df[\"Obs_Node_Name\"]))\n",
    "    num_obs_witness = len(obs_witnesses) \n",
    "    obs_wit_col.append(num_obs_witness)\n",
    "    \n",
    "    # Count IPs\n",
    "    peers_reported = list(set(this_height_df[\"Peer_IP\"]))\n",
    "    num_peers_reported = len(peers_reported) \n",
    "    peers_col.append(num_peers_reported)   \n",
    "    \n",
    "summary_df['Height'] = height_col\n",
    "# summary_df['Hash'] = hash_col\n",
    "summary_df['First_Time'] = first_col\n",
    "summary_df['Prop_Time'] = prop_time_col\n",
    "summary_df['Obs_Witness_Count'] = obs_wit_col\n",
    "summary_df['Peers_Report_Count'] = peers_col\n",
    "summary_df['Prct_Prop_Time'] = prct_prop_time_col\n",
    "# summary_df['Sorted_Timestamps'] = timestamps_col\n",
    "\n",
    "\n",
    "\n",
    "    # if num_obs_witness >= min_obs_witness:\n",
    "    #     if qverbose > 2: print('kept with ' + str(num_obs_witness) + ' witnesses')\n",
    "    #     witness_df = witness_df.append(this_height_df)\n",
    "    # else:\n",
    "    #     if qverbose > 2: print('rejected with only ' + str(num_obs_witness) + ' witnesses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability density function plot\n",
    "\n",
    "First, let's look at histograms of the global propagation time.\n",
    "\n",
    "The first plot uses linear axes, and the second plot shows the same thing with log axes.\n",
    "\n",
    "This is called the \"probability density function\" (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear plot\n",
    "height_hist = plt.figure(figsize=(10,5), facecolor='w')\n",
    "plt.hist(summary_df['Prop_Time'], bins=100)\n",
    "plt.xlabel('last-first timestamp (s)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed block propagation time (4 global nodes)')\n",
    "\n",
    "# Log plot\n",
    "height_hist = plt.figure(figsize=(10,5), facecolor='w')\n",
    "hdata = summary_df['Prop_Time']\n",
    "plt.hist(summary_df['Prop_Time'], bins=np.logspace(np.log10(min(hdata)+0.1),np.log10(max(hdata)),100));\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('last-first timestamp (s)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed block propagation time (4 global nodes)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative distribution function plot\n",
    "\n",
    "Now let's look at a closely-related plot, the \"cumulative distribution function\" (CDF)\n",
    "\n",
    "Crosshairs have been added to show how to interpret this plot. (Note that when the data are updated, the static crosshairs may not match up with the plot / data anymore)\n",
    "\n",
    "For a given crosshair, <y-axis coordinate> fraction of blocks propagate in under <x-axis coordinate> seconds\n",
    "    \n",
    "Black line shows that 50% of blocks propagate in under 250 ms. Red line shows that 85% of blocks propagate in under 850 ms (meaning that 15% of Zcash blocks take more than 800 ms to propagate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot\n",
    "\n",
    "height_hist = plt.figure(figsize=(10,5), facecolor='w')\n",
    "plt.hist(summary_df['Prop_Time'], bins=np.logspace(np.log10(min(hdata)+0.1),np.log10(max(hdata)),100), cumulative=True, density=True);\n",
    "#plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('last-first timestamp (s)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed block propagation time (4 global nodes)')\n",
    "\n",
    "\n",
    "# Add labels to explain\n",
    "plt.axvline(x=12, color='black')\n",
    "plt.axhline(y=0.5, color='black')\n",
    "plt.axvline(x=25, color='red')\n",
    "plt.axhline(y=0.85, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot\n",
    "\n",
    "Now let's plot each block as a dot where x-axis is the block height, and y-axis is the observed global propagation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5), facecolor='w')\n",
    "plt.scatter(x=summary_df['Height'], y=summary_df['Prop_Time'])\n",
    "plt.xlabel('last-first timestamp (s)')\n",
    "plt.ylabel('Global propagation time (s)')\n",
    "plt.title('Observed block propagation time ('+str(len(node_names))+' global nodes)');\n",
    "rolling_mean = summary_df.Prop_Time.rolling(window=median_window).mean()\n",
    "plt.scatter(summary_df.Height, rolling_mean, label='hour rolling window', color='orange')\n",
    "plt.axvline(x=start_height, color='green')\n",
    "plt.axvline(x=stop_height, color='red')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,5), facecolor='w')\n",
    "plt.scatter(x=summary_df['Height'], y=summary_df['Prop_Time'], c=summary_df['Peers_Report_Count'])\n",
    "plt.xlabel('last-first timestamp (s)')\n",
    "plt.ylabel('Global propagation time (s)')\n",
    "plt.title('Observed block propagation time ('+str(len(node_names))+' global nodes)');\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10), facecolor='w')\n",
    "plt.scatter(x=summary_df['Height'], y=summary_df['Prct_Prop_Time'])\n",
    "plt.xlabel('last-first timestamp (s)')\n",
    "plt.ylabel('Global propagation time (s)')\n",
    "plt.title('Observed block '+ str(percentile_val) + '% propagation time ('+str(len(node_names))+' observatories)');\n",
    "rolling_mean = summary_df.Prct_Prop_Time.rolling(window=median_window).mean()\n",
    "plt.scatter(summary_df.Height, rolling_mean, label='hour rolling window', color='orange')\n",
    "plt.axvline(x=start_height, color='green')\n",
    "plt.axvline(x=stop_height, color='red')\n",
    "summary_df['Prct_Prop_Time']\n",
    "\n",
    "fig = plt.figure(figsize=(15,10), facecolor='w')\n",
    "plt.scatter(x=summary_df['Height'], y=summary_df['Prct_Prop_Time'])\n",
    "plt.xlabel('last-first timestamp (s)')\n",
    "plt.ylabel('Global propagation time (s)')\n",
    "plt.title('Observed block '+ str(percentile_val) + '% propagation time ('+str(len(node_names))+' observatories)');\n",
    "plt.scatter(summary_df.Height, rolling_mean, label='hour rolling window', color='orange')\n",
    "plt.axvline(x=start_height, color='green')\n",
    "#plt.axvline(x=stop_height, color='red')\n",
    "plt.ylim((0,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap plots\n",
    "(same concept as the scatter plots above, except shaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Heatmap(x=summary_df['Height'], y=summary_df['Prop_Time'], ymax=75, vmax=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "Data frame to CSV if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if savedata:\n",
    "    summary_df.to_csv('summary_df.csv', index_label = 'block_hash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
