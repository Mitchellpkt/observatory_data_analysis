{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Zcash Observatory data\n",
    "\n",
    "Mitchell Krawiec-Thayer and Pranav Thirunavukkarasu\n",
    "\n",
    "June 2020\n",
    "\n",
    "Observatory R & D at Insight supported by the Zcash Foundation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files = '.'\n",
    "savedata = 1\n",
    "remove_sync_hack = 0\n",
    "sync_threshold = 5 # s\n",
    "qverbose = 1\n",
    "min_obs_witness = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pandas as pd;\n",
    "import math;\n",
    "import os;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates heatmap visualizations\n",
    "\n",
    "def Heatmap(x, y, LinBins = (60,60), LogBins = (60,60), title = '', xlabel = '', ylabel = '', yscale = 'linear', xscale = 'linear', onlyplot = '', vmax = 'auto', vmin = 0, clabel='', ymax = 'auto'):\n",
    "    # import numpy as np\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # MPKT 2019.06\n",
    "\n",
    "    # Note that this is a hacky function that cannot handle NaNs at the moment\n",
    "    \n",
    "    if ymax == 'auto':\n",
    "        ymax_val = np.log10(int(np.max(x)))\n",
    "    else:\n",
    "        ymax_val = np.log10(int(ymax))\n",
    "    \n",
    "    # Log plot\n",
    "    if not onlyplot == 'linear':\n",
    "        fig = plt.figure(figsize=(15,5), facecolor='white')\n",
    "        if len(LogBins) == 2:\n",
    "            yedges = np.logspace(np.log10(1),ymax_val, LogBins[1])\n",
    "        H, xedges, yedges = np.histogram2d(list(x),list(y), bins=(LogBins[1],yedges))\n",
    "        \n",
    "        # H needs to be rotated and flipped\n",
    "        H = np.rot90(H)\n",
    "        H = np.flipud(H)\n",
    "        \n",
    "        if vmax == 'auto':\n",
    "            vmax = np.max(H)\n",
    "        # Plot 2D histogram using pcolor\n",
    "        plt.pcolormesh(xedges,yedges,H,vmax=vmax, vmin=vmin)\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.ax.set_ylabel(clabel)                                          \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.xscale(xscale)\n",
    "        plt.yscale(yscale)\n",
    "        plt.show()\n",
    "        \n",
    "        return fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Assumes that data is in directories named by node ID (location)\n",
    "\n",
    "The cell that reads in CSV files also heavily processes, and may take minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_names:\n",
      "['mumbai', 'virginia', 'london', 'africa']\n"
     ]
    }
   ],
   "source": [
    "# What are the nearby directories\n",
    "folders = [x[0] for x in os.walk(path_to_files)]\n",
    "\n",
    "# Initialize\n",
    "node_names = list()\n",
    "for f in range(len(folders)):\n",
    "    this_folder_raw = folders[f]\n",
    "    if not (this_folder_raw[0:3] == './.' or this_folder_raw == '.'):\n",
    "        node_names.append(this_folder_raw[2::])\n",
    "print('node_names:')\n",
    "print (node_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data from all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data, iterating over nodes\n",
    "\n",
    "timestamps_df = pd.DataFrame()\n",
    "heights_df = pd.DataFrame()\n",
    "\n",
    "for f in range(len(node_names)):\n",
    "    this_node_name = node_names[f]\n",
    "\n",
    "    # Import height data\n",
    "    blocks_file_name = os.path.join(this_node_name,'blocks_v1.csv')\n",
    "    temp_df1 = pd.read_csv(blocks_file_name, index_col = 'Hash')\n",
    "    temp_df1 = temp_df1.filter([\"Hash\", \"Height\"], axis=1)\n",
    "    temp_df1['Obs_Node_Name'] = this_node_name\n",
    "    heights_df = heights_df.append(temp_df1)\n",
    "\n",
    "    # Import timestamp data\n",
    "    heights_file_name = os.path.join(this_node_name,'inv_v1.csv')\n",
    "    temp_df2 = pd.read_csv(heights_file_name, index_col = 'Hash')\n",
    "    timestamps_df = timestamps_df.append(temp_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join heights\n",
    "all_data = timestamps_df.join(heights_df.drop_duplicates(), on='Hash') # temp hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer piecewise analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all peers that we saw?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_peerlist = list(set(all_data['Peer_IP']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove sync data (loop over peers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peer 1 of 222\n",
      "Peer 2 of 222\n",
      "Peer 3 of 222\n",
      "Peer 4 of 222\n",
      "Peer 5 of 222\n"
     ]
    }
   ],
   "source": [
    "heights_to_keep = list()\n",
    "heights_to_mask = list()\n",
    "non_sync_data = pd.DataFrame()\n",
    "\n",
    "for p in range(5): #len(global_peerlist)):\n",
    "    if qverbose: print(\"Peer \" + str(p+1) + ' of ' + str(len(global_peerlist)))\n",
    "    this_IP = global_peerlist[p]\n",
    "    this_peer_data = all_data[all_data['Peer_IP'] == this_IP]\n",
    "    this_peer_data = this_peer_data.reset_index()\n",
    "    heights_seen_raw = list(set(this_peer_data['Height']))\n",
    "    heights_seen = [x for x in heights_seen_raw if not np.isnan(x)]\n",
    "    \n",
    "    # De-dupe heights\n",
    "    first_heard_df = this_peer_data.groupby(['Height'])['Validated_Time'].min().to_frame().reset_index()\n",
    "    \n",
    "    # Split into parallel lists\n",
    "    height_array = list(first_heard_df[\"Height\"])\n",
    "    time_array = list(first_heard_df[\"Validated_Time\"])\n",
    "\n",
    "    for h in range(len(height_array)):\n",
    "        # get data for this block\n",
    "        this_height = height_array[h]\n",
    "        this_time = time_array[h]\n",
    "        \n",
    "        # did the previous block exist\n",
    "        last_height = this_height - 1\n",
    "        try:\n",
    "            last_height_index = height_array.index(last_height)\n",
    "            last_time = time_array[last_height_index]\n",
    "            \n",
    "            if this_time - last_time > sync_threshold:\n",
    "                heights_to_keep.append(this_height)\n",
    "            else:\n",
    "                # previous block was too recent\n",
    "                heights_to_mask.append(this_height)\n",
    "        except:\n",
    "            # didn't see previous block\n",
    "            heights_to_mask.append(this_height)\n",
    "            \n",
    "\n",
    "    # Okay, now remove those rows from the all-node this-peer data \n",
    "    for h in range(len(heights_to_mask)):\n",
    "        this_height = heights_to_mask[h]\n",
    "        idx = this_peer_data.index[this_peer_data['Height'] == this_height].tolist()\n",
    "        \n",
    "        if len(idx) > 0:\n",
    "            if len(idx) > 1:\n",
    "                for i in range(len(idx)):\n",
    "                    this_peer_data.iat[i,1] = np.nan \n",
    "            else: # len(idx) == 1:\n",
    "                this_peer_data.iat[idx[0],1] = np.nan \n",
    "\n",
    "    this_peer_data = this_peer_data.dropna()\n",
    "    \n",
    "    non_sync_data = non_sync_data.append(this_peer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(columns=['Height','Hash','First_Time','Prop_Time','Obs_Witness_Count','Peers_Report_Count','Sorted_Timestamps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 7055\n",
      "100 of 7055\n",
      "200 of 7055\n",
      "300 of 7055\n",
      "400 of 7055\n",
      "500 of 7055\n",
      "600 of 7055\n",
      "700 of 7055\n",
      "800 of 7055\n",
      "900 of 7055\n",
      "1000 of 7055\n",
      "1100 of 7055\n",
      "1200 of 7055\n",
      "1300 of 7055\n",
      "1400 of 7055\n",
      "1500 of 7055\n",
      "1600 of 7055\n",
      "1700 of 7055\n",
      "1800 of 7055\n",
      "1900 of 7055\n",
      "2000 of 7055\n",
      "2100 of 7055\n",
      "2200 of 7055\n",
      "2300 of 7055\n",
      "2400 of 7055\n",
      "2500 of 7055\n",
      "2600 of 7055\n",
      "2700 of 7055\n",
      "2800 of 7055\n",
      "2900 of 7055\n",
      "3000 of 7055\n",
      "3100 of 7055\n",
      "3200 of 7055\n",
      "3300 of 7055\n",
      "3400 of 7055\n",
      "3500 of 7055\n",
      "3600 of 7055\n",
      "3700 of 7055\n",
      "3800 of 7055\n",
      "3900 of 7055\n",
      "4000 of 7055\n",
      "4100 of 7055\n",
      "4200 of 7055\n",
      "4300 of 7055\n",
      "4400 of 7055\n",
      "4500 of 7055\n",
      "4600 of 7055\n",
      "4700 of 7055\n",
      "4800 of 7055\n",
      "4900 of 7055\n",
      "5000 of 7055\n",
      "5100 of 7055\n",
      "5200 of 7055\n",
      "5300 of 7055\n",
      "5400 of 7055\n",
      "5500 of 7055\n",
      "5600 of 7055\n",
      "5700 of 7055\n",
      "5800 of 7055\n",
      "5900 of 7055\n",
      "6000 of 7055\n",
      "6100 of 7055\n",
      "6200 of 7055\n",
      "6300 of 7055\n",
      "6400 of 7055\n",
      "6500 of 7055\n",
      "6600 of 7055\n",
      "6700 of 7055\n",
      "6800 of 7055\n",
      "6900 of 7055\n",
      "7000 of 7055\n"
     ]
    }
   ],
   "source": [
    "witness_df = pd.DataFrame()\n",
    "global_height_list_raw = list(set(all_data['Height']))\n",
    "global_height_list = [x for x in global_height_list_raw if not np.isnan(x)]\n",
    "summary_df = pd.DataFrame(columns=['Height','Hash','First_Time','Prop_Time','Obs_Witness_Count','Peers_Report_Count','Sorted_Timestamps'])\n",
    "\n",
    "height_col = list()\n",
    "hash_col = list()\n",
    "first_col = list()\n",
    "prop_time_col = list()\n",
    "obs_wit_col = list()\n",
    "peers_col = list()\n",
    "timestamps_col = list()\n",
    "\n",
    "for h in range(len(global_height_list)):\n",
    "    if qverbose > 0: \n",
    "        if h % 100 == 0:\n",
    "            print(str(h) + ' of ' + str(len(global_height_list)))\n",
    "    this_height = global_height_list[h]\n",
    "    this_height_df = non_sync_data[non_sync_data[\"Height\"] == this_height]\n",
    "    height_col.append(this_height)\n",
    "    # hash_col.append()\n",
    "    \n",
    "    # Count witnesses\n",
    "    obs_witnesses = list(set(this_height_df[\"Obs_Node_Name\"]))\n",
    "    num_obs_witness = len(obs_witnesses) \n",
    "    obs_wit_col.append(num_obs_witness)\n",
    "    \n",
    "    # Count IPs\n",
    "    peers_reported = list(set(this_height_df[\"Peer_IP\"]))\n",
    "    num_peers_reported = len(peers_reported) \n",
    "    peers_col.append(num_peers_reported)\n",
    "    \n",
    "    # Timing\n",
    "    try: \n",
    "        min_time = min(this_height_df['Validated_Time'])\n",
    "        max_time = max(this_height_df['Validated_Time'])\n",
    "        prop_time = max_time - min_time\n",
    "    except:\n",
    "        min_time = np.nan\n",
    "        prop_time = np.nan\n",
    "        \n",
    "    first_col.append(min_time)\n",
    "    prop_time_col.append(prop_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "summary_df['Height'] = height_col\n",
    "# summary_df['Hash'] = hash_col\n",
    "summary_df['First_Time'] = first_col\n",
    "summary_df['Prop_Time'] = prop_time_col\n",
    "summary_df['Obs_Witness_Count'] = obs_wit_col\n",
    "summary_df['Peers_Report_Count'] = peers_col\n",
    "# summary_df['Sorted_Timestamps'] = timestamps_col\n",
    "\n",
    "\n",
    "\n",
    "    # if num_obs_witness >= min_obs_witness:\n",
    "    #     if qverbose > 2: print('kept with ' + str(num_obs_witness) + ' witnesses')\n",
    "    #     witness_df = witness_df.append(this_height_df)\n",
    "    # else:\n",
    "    #     if qverbose > 2: print('rejected with only ' + str(num_obs_witness) + ' witnesses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Hash</th>\n",
       "      <th>First_Time</th>\n",
       "      <th>Prop_Time</th>\n",
       "      <th>Obs_Witness_Count</th>\n",
       "      <th>Peers_Report_Count</th>\n",
       "      <th>Sorted_Timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851975.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.591051e+09</td>\n",
       "      <td>0.251</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>851976.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.591051e+09</td>\n",
       "      <td>0.157</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>851977.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.591051e+09</td>\n",
       "      <td>0.663</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>851978.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.591051e+09</td>\n",
       "      <td>1.543</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>851979.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.591051e+09</td>\n",
       "      <td>1.931</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Height Hash    First_Time  Prop_Time  Obs_Witness_Count  \\\n",
       "0  851975.0  NaN  1.591051e+09      0.251                  4   \n",
       "1  851976.0  NaN  1.591051e+09      0.157                  4   \n",
       "2  851977.0  NaN  1.591051e+09      0.663                  4   \n",
       "3  851978.0  NaN  1.591051e+09      1.543                  4   \n",
       "4  851979.0  NaN  1.591051e+09      1.931                  4   \n",
       "\n",
       "   Peers_Report_Count Sorted_Timestamps  \n",
       "0                   4               NaN  \n",
       "1                   4               NaN  \n",
       "2                   4               NaN  \n",
       "3                   4               NaN  \n",
       "4                   4               NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty buffers\n",
    "prop_list = list()\n",
    "min_list = list()\n",
    "max_list = list()\n",
    "\n",
    "# Loop over heights\n",
    "for h in range(len(global_df)):\n",
    "    min_stamps = list()\n",
    "    max_stamps = list()\n",
    "    keep_data = 1\n",
    "    \n",
    "    # Loop over nodes\n",
    "    for loc_ind in range(len(node_names)):\n",
    "        min_val_col = global_df[node_names[loc_ind]+\"_min\"]\n",
    "        min_val = min_val_col[h]\n",
    "        min_stamps.append(min_val)\n",
    "        max_val_col = global_df[node_names[loc_ind]+\"_max\"]\n",
    "        max_val = max_val_col[h]\n",
    "        max_stamps.append(max_val)\n",
    "        \n",
    "        ############\n",
    "        # FILTERING\n",
    "        ###########\n",
    "        \n",
    "        # Filter 1\n",
    "        # throw out if any node only heard from one peer\n",
    "        # (toggle off by commenting out logic block)\n",
    "        # MPKT note: This might be unnecessary, but doesn't hurt\n",
    "        if max_val - min_val == 0:\n",
    "            keep_data = 0 # throw out if any node only heard from ONE peer\n",
    "        \n",
    "        # Filter 2\n",
    "        # throw out if ANY node did not hear the block\n",
    "        # (toggle off by commenting out logic block)\n",
    "        if math.isnan(max_val):\n",
    "            keep_data = 0 \n",
    "    \n",
    "    # Keep track \n",
    "    global_min = min(min_stamps)\n",
    "    min_list.append(global_min)\n",
    "    global_max = max(max_stamps)\n",
    "    max_list.append(global_max)\n",
    "    global_prop = global_max - global_min\n",
    "    \n",
    "    if keep_data:\n",
    "        prop_list.append(global_prop)\n",
    "    else:\n",
    "        prop_list.append(np.nan)\n",
    "    \n",
    "# Transfer from bufffers to data frame\n",
    "global_df['global_min'] = min_list\n",
    "global_df['global_max'] = max_list\n",
    "global_df['global_prop_time'] = prop_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaNs\n",
    "backup_global_df = global_df # temp\n",
    "global_df = global_df[global_df['global_prop_time']>0]\n",
    "global_df_clean = global_df.dropna();\n",
    "global_df_clean = global_df_clean.sort_values(by='Height')\n",
    "temp_clean = global_df_clean # tempp\n",
    "print(\"Number of blocks: \" + str(len(global_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacky sync fix\n",
    "\n",
    "To eliminate artifacts from syncing nodes, we can throw out any data point whose prop time was less than the data point before it.\n",
    "\n",
    "Assuming that the prop time of block N+1 doesn't depend on prop time of block N (which I believe to be true) then there's a 50/50 chance that a prop_time(N+1) < prop_time(N)\n",
    "\n",
    "And for a sync, there's 100% chance that prop_time(N+1) < prop_time(N)\n",
    "\n",
    "So if we throw out every data point that satisfies \"prop_time(N+1) < prop_time(N)\" we will eliminate 100% of the artifacts, and 50% of the legit data points\n",
    "\n",
    "This is statistical duct tape and should be replaced with a better method, but for now it should remove artifacts from the data set without introducing bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "global_df_clean = temp_clean\n",
    "hack_epsilon = 0\n",
    "print(\"Sample size: \" + str(len(global_df_clean)))\n",
    "\n",
    "if remove_sync_hack:\n",
    "    \n",
    "    ## Show extant data\n",
    "    fig = plt.figure(figsize=(10,5), facecolor='w')\n",
    "    plt.scatter(x=global_df_clean['Height'], y=global_df_clean['global_prop_time'], color='red')\n",
    "    plt.xlabel('PRE FILTER')\n",
    "    plt.ylabel('Global propagation time (ms)')\n",
    "    plt.title('Observed block propagation time ('+str(len(node_names))+' global nodes)');\n",
    "    \n",
    "    temp_df = global_df_clean.diff()\n",
    "    global_df_clean = global_df_clean[temp_df['global_prop_time']>hack_epsilon]\n",
    "    \n",
    "    plt.scatter(x=global_df_clean['Height'], y=global_df_clean['global_prop_time'], color='blue')\n",
    "\n",
    "    print(\"Sample size: \" + str(len(global_df_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability density function plot\n",
    "\n",
    "First, let's look at histograms of the global propagation time.\n",
    "\n",
    "The first plot uses linear axes, and the second plot shows the same thing with log axes.\n",
    "\n",
    "This is called the \"probability density function\" (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear plot\n",
    "height_hist = plt.figure(figsize=(10,5), facecolor='w')\n",
    "plt.hist(global_df_clean['global_prop_time'], bins=100)\n",
    "plt.xlabel('last-first timestamp (ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed block propagation time (4 global nodes)')\n",
    "\n",
    "# Log plot\n",
    "height_hist = plt.figure(figsize=(10,5), facecolor='w')\n",
    "plt.hist(global_df_clean['global_prop_time'], bins=np.logspace(np.log10(min(global_df_clean['global_prop_time'][global_df_clean['global_prop_time']>0])),np.log10(max(global_df_clean['global_prop_time'])), 100) );plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('last-first timestamp (ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed block propagation time (4 global nodes)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative distribution function plot\n",
    "\n",
    "Now let's look at a closely-related plot, the \"cumulative distribution function\" (CDF)\n",
    "\n",
    "Crosshairs have been added to show how to interpret this plot. (Note that when the data are updated, the static crosshairs may not match up with the plot / data anymore)\n",
    "\n",
    "For a given crosshair, <y-axis coordinate> fraction of blocks propagate in under <x-axis coordinate> seconds\n",
    "    \n",
    "Black line shows that 50% of blocks propagate in under 20 ms. Red line shows that 85% of blocks propagate in under 300 ms (meaning that 15% of Zcash blocks take more than 800 ms to propagate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot\n",
    "\n",
    "height_hist = plt.figure(figsize=(10,5), facecolor='w')\n",
    "plt.hist(global_df_clean['global_prop_time'], bins=np.logspace(np.log10(min(global_df_clean['global_prop_time'][global_df_clean['global_prop_time']>0])),np.log10(max(global_df_clean['global_prop_time'])), 100), cumulative=True, density=True);\n",
    "#plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('last-first timestamp (ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed block propagation time (4 global nodes)')\n",
    "\n",
    "\n",
    "# Add labels to explain\n",
    "plt.axvline(x=18, color='black')\n",
    "plt.axhline(y=0.5, color='black')\n",
    "plt.axvline(x=300, color='red')\n",
    "plt.axhline(y=0.85, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot\n",
    "\n",
    "Now let's plot each block as a dot where x-axis is the block height, and y-axis is the observed global propagation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5), facecolor='w')\n",
    "plt.scatter(x=global_df_clean['Height'], y=global_df_clean['global_prop_time'])\n",
    "plt.xlabel('last-first timestamp (ms)')\n",
    "plt.ylabel('Global propagation time (ms)')\n",
    "plt.title('Observed block propagation time ('+str(len(node_names))+' global nodes)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, we see these downward-sloping strings of blocks when the \"last\" timestamp is from a node that booted up later and requested old blocks. Thus, these are anomalous data points (not representative of propagation time *at time of broadcast*) so we should figure out a way to filter these out. Let's zoom in to see the real propagation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_hist = plt.figure(figsize=(15,5), facecolor='w')\n",
    "plt.scatter(x=global_df_clean['Height'], y=global_df_clean['global_prop_time'])\n",
    "plt.xlabel('last-first timestamp (ms)')\n",
    "plt.ylabel('Global propagation time (ms)')\n",
    "plt.title('Observed block propagation time ('+str(len(node_names))+' global nodes)');\n",
    "plt.ylim([0,80])\n",
    "\n",
    "# Optional, add lines:\n",
    "# tempdf = global_df_clean.sort_values(by='Height')\n",
    "# plt.plot(tempdf['Height'], tempdf['global_prop_time'],color='gray', linewidth=1, linestyle='dashed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap plots\n",
    "(same concept as the scatter plots above, except shaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Heatmap(x=global_df_clean['Height'], y=global_df_clean['global_prop_time'], ymax=50, vmax=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "Data frame to CSV if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if savedata:\n",
    "    global_df_clean.to_csv('global_df_clean.csv', index_label = 'block_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
